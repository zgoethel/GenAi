{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "Ollama": {
    // Gemma 27b + llama3.2:1b can fit in 24GB VRAM
    "Endpoint": "http://localhost:11434/",
    "Model": "gemma3:27b-it-qat",
    "VisionEndpoint": "http://localhost:11434/",
    "VisionModel": "gemma3:27b-it-qat",
    "CheapEndpoint": "http://localhost:11434/",
    "CheapModel": "llama3.2:1b",
    // I run embedding on CPU in a separate Docker host
    "EmbedEndpoint": "http://localhost:11434/",
    "EmbedModel": "mxbai-embed-large",
    "MaxConcurrent": 2
  },
  "WebUi": {
    "Endpoint": "http://localhost:7860/"
  },
  "Passphrase": "admin"
}